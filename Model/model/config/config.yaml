# config 템플릿 (수정 X)
name: # Train_name
seed: # seed(42)
train:
  model_name: # backbone model like 'beomi/KoAlpaca-Polyglot-12.8B'
  epoch: 3
  batch_size: 8
  LR:
    name: WarmupDecayLR # LambdaLR, StepLR, CyclicLR, ExponentialLR, WarmupConstantLR , WarmupDecayLR
    lr: 0.0001
    base: 20 # CyclicLR를 쓴다면, LR에 대한 min_lr을 적어주세요(20 == LR/20)
    max: 5 # CyclicLR를 쓴다면, LR에 대한 max_lr을 적어주세요(5 == LR/5)
    step_up: 5 # CyclicLR를 쓴다면, warmup steps를 적어주세요
    step_down: 5 # CyclicLR를 쓴다면, cooldown steps를 적어주세요 (단, up+down은 epoch과 동일해야 함)
    warmupconstantLR_step: 3
    warmupdecayLR_warmup: 1 # step 기준으로 계산
    warmupdecayLR_total: 100000 # step 기준으로 계산
    interval: step # epoch
  token_max_len: 512
  halfprecision: True
  gradient_accumulation: 1    # 배치사이즈=16 이고 이 옵션이 2일때, batch=32와 같은 효과.
  test_size: 0.1
data_name: # dataset's name in Data/Dataset/ready
max_sample_num: # Sample up to this number of training samples
select_DC:
  -  # data_cleaning_demo
select_DA:
  -  # data_augmentation_demo
adapt:
  peft: QLoRA #,LoRAFull, LoRAfp16, original
  r: 8
  alpha: 8
  dropout: 0.05
wandb:
  id: # wandb_id